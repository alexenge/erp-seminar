{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820f3b08",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Statistics\n",
    "\n",
    "In the previous chapters, we already got an idea about the differences in EEG activity between our conditions of interest (e.g., faces versus cars), as shown in the time course and scalp topography plots.\n",
    "However, we have to model the data statistically to be able to quantify the size of this difference and if it is statistically significant (i.e., not due to chance).\n",
    "\n",
    "In this chapter, we will encounter different statistical tests that can be used to test hypotheses about the data.\n",
    "While the EEG processing is done in Python as before, we will use the R programming language for statistical modeling because it has a larger number of statistical functions and packages, and is widely used in the psychological science community.\n",
    "\n",
    "```{admonition} Goals\n",
    ":class: note\n",
    "\n",
    "* Apply \"classical\" models based on averaged data (e.g., $t$-tests, ANOVA)\n",
    "* Apply linear mixed-effects models to the single trial data\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56dc6f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Load Python packages\n",
    "\n",
    "We'll use the hu-neuro-pipeline package introduced in Chapter 5 (Pipeline) for EEG processing, and Numpy and seaborn for post-processing and plotting.\n",
    "As mentioned before, the actual statistical modeling will be done in R, but there are also Python packages for this (e.g., [statsmodels](https://www.statsmodels.org/stable/index.html)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pipeline import group_pipeline\n",
    "from pipeline.datasets import get_erpcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ce312",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## EEG processing pipeline\n",
    "\n",
    "We use the same processing pipeline as introduced in Chapter 5 (Pipeline), giving us the single trial data and the average time courses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7bdcb",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "files_dict = get_erpcore('N170', participants=10, path='data')\n",
    "\n",
    "trials, evokeds, config = group_pipeline(raw_files=files_dict['raw_files'],\n",
    "                                         log_files=files_dict['log_files'],\n",
    "                                         output_dir='output',\n",
    "                                         montage='biosemi64',\n",
    "                                         ica_method='fastica',\n",
    "                                         ica_n_components=15,\n",
    "                                         triggers=range(1, 81),\n",
    "                                         skip_log_conditions={'value': range(81, 203)},\n",
    "                                         components={'name': 'N170',\n",
    "                                                     'tmin': 0.110,\n",
    "                                                     'tmax': 0.150,\n",
    "                                                     'roi': ['PO8']},\n",
    "                                         average_by={'face': 'value <= 40',\n",
    "                                                     'car': 'value > 40'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce3fa2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Single trial data\n",
    "\n",
    "The main output of the hu-neuro-pipeline package is the single trial data frame, which contains the EEG data for each trial, averaged across an *a priori* hypothesized time window and electrode(s) of interest (see the `components` argument above).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723fc54",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Using a combination of pandas and Numpy, we'll create a new column in the data frame with verbal labels for our two conditions of interest (faces and cars).\n",
    "This is based on the numerical event codes (stored in the `value` column), the meaning of which was described in Chapter 3 (Epoching).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a70b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials['condition'] = np.where(trials['value'] <= 40, 'face', 'car')\n",
    "trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d37cab",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Using seaborn, we can plot the distribution of the single trial N170 amplitudes, separately for the two conditions.\n",
    "Note that this plot does not take into account the repeated measurements of the same participant, which we will address later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.violinplot(data=trials, y='N170', hue='condition',\n",
    "                   inner='quart', split=True, fill=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181c5bf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Linear models\n",
    "\n",
    "The \"traditional\" way for statistical analysis of ERPs is to (a) average the data across trials for each participant and condition, and (b) apply a statistical test to the averaged data.\n",
    "Let's start with the first step.\n",
    "The pandas package has the necessary methods to group the data by participant and condition (`groupby()` method), and compute the average N170 amplitude across trials for each grouping (`mean()` method).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8865a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_ave = trials[['participant_id', 'condition', 'N170']].\\\n",
    "    groupby(['participant_id', 'condition']).\\\n",
    "    mean().\\\n",
    "    reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a804acf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we can pass the data frame to R and apply an appropriate statistical test.\n",
    "Using the `rpy2` package, we can run R code directly in the Jupyter notebook, using the `%%R` magic command at the beginning of a code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be758319",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44f390",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "An appropriate statistical test needs to take into account that our two conditions are manipulated *within* participants, that is, we have repeated measures that are likely correlated with one another (violating the independence assumption of many statistical tests, e.g., linear regression).\n",
    "\n",
    "Luckily, there are statistical tests that can handle repeated measures data, such as the paired $t$-test or repeated measures ANOVA.\n",
    "\n",
    "Let's start with the paired $t$-test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78288dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i trials_ave\n",
    "t.test(N170 ~ condition, data = trials_ave, paired=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9a05a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We see that in this sample, the amplitude in response to faces is approximately 2.5 ÂµV lower (more negative) than in response to cars, as would be expected for the N170 component.\n",
    "This difference is statistically significant with $t(9) = -6.6$, $p \\approx .0001$.\n",
    "\n",
    "Note that we could have gotten the same result by applying a one sample $t$-test to the difference scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada485b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_ave_wide = trials_ave.pivot(index='participant_id', columns='condition', \n",
    "                                   values='N170')\n",
    "trials_ave_wide['diff'] = trials_ave_wide['car'] - trials_ave_wide['face']\n",
    "trials_ave_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i trials_ave_wide\n",
    "t.test(trials_ave_wide$diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392883e",
   "metadata": {},
   "source": [
    "Or by running a repeated measures ANOVA with a single (two-level) factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4badd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i trials_ave\n",
    "\n",
    "# install.packages(\"ez\")\n",
    "\n",
    "ez::ezANOVA(\n",
    "  data = trials_ave,\n",
    "  dv = N170,\n",
    "  wid = participant_id,\n",
    "  within = condition\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c64a6c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Linear mixed-effects models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b11034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i trials\n",
    "\n",
    "# install.packages(\"lme4\")\n",
    "\n",
    "mod <- lme4::lmer(N170 ~ 1 + condition + (1 | participant_id), trials)\n",
    "summary(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46dfe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i trials\n",
    "\n",
    "# install.packages(\"lmerTest\")\n",
    "\n",
    "mod <- lmerTest::lmer(N170 ~ 1 + condition + (1 | participant_id), trials)\n",
    "summary(mod)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
